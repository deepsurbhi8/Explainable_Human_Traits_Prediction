# -*- coding: utf-8 -*-
"""LSTM_on_Audio_chunk_level_Overall_Local.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ehp8axN02M2sajO_nnDZ5g2oGTi9HKWM
"""

import numpy as np
import pandas as pd
import glob
import os
import random
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from math import sqrt
import matplotlib.pyplot as plt
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import mean_absolute_error
from sklearn import metrics
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.decomposition import PCA
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
import scipy.io as sio
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import regularizers
from tensorflow.keras.utils import to_categorical

from sklearn import preprocessing

from google.colab import drive
drive.mount('/content/drive')

def model_call(X_data,y_data,chunk_time,feature_size,label_name):
  nKineme, seqLen, nClass = feature_size, chunk_time-1, 1 #num of kineme, seqlength for LSTM to be consider and num of output classes
  
  #Model Formation
  from keras.models import Sequential
  from keras.layers import Dense
  from keras.layers import LSTM
  from keras.layers import Dropout
  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
  Model = Sequential()
  Model.add(LSTM(20,activation="tanh",dropout=0.2,recurrent_dropout=0.0,input_shape=(seqLen, feature_size)))
  Model.add(Dense(units = nClass, activation='linear'))
  opt = keras.optimizers.Adam(learning_rate=0.01)
  Model.compile(optimizer = opt, loss = 'mean_absolute_error')
  Model.summary()

  #Lists to have PCC and MAE
  train_mae=[]
  test_mae =[]
  train_PCC =[]
  test_PCC = []
  train_acc =[]
  test_acc = []

  n=1
  #randnums= np.random.randint(1,file_to_work.shape[0])   #To make train and test fold
  random_state = 42
  rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=random_state)     #repeat kfold function
  for train_idx, test_idx in rkf.split(X_data):
      train_features, test_features, train_labels, test_labels = X_data[train_idx], X_data[test_idx], y_data[train_idx], y_data[test_idx] 
      train_aus = train_features.reshape((train_features.shape[0], seqLen, nKineme))
      # print("Train Data:{0}".format(train_aus))
      test_aus = test_features.reshape((test_features.shape[0], seqLen, nKineme))
      # print("Test Data:{0}".format(test_aus))
      print(train_aus.shape, test_aus.shape, train_labels.shape, test_labels.shape)
      zero_bias_history = Model.fit(train_aus, train_labels, epochs = 30, batch_size = 32, validation_split=0.1,callbacks=[callback])  #Fitting the model      
      #train predictions
      y_pred_train = Model.predict(train_aus)
      y_pred_train = np.around(y_pred_train,3)     
      y_pred_test = Model.predict(test_aus)
      y_pred_test = np.around(y_pred_test,3)
      train_mae.append(mean_absolute_error(train_labels, y_pred_train)) ##mean squarred train error
      test_mae.append(mean_absolute_error(test_labels, y_pred_test)) #mean squarred test error
      train_acc.append(1-mean_absolute_error(train_labels, y_pred_train))
      test_acc.append(1-mean_absolute_error(test_labels, y_pred_test))
      y_train = train_labels.reshape(-1,1)
      b = np.corrcoef(y_train.T,y_pred_train.T)
      train_PCC.append(b[0][1])
      y = test_labels.reshape(-1,1)
      a = np.corrcoef(y.T,y_pred_test.T)
      test_PCC.append(a[0][1])
      print(n)
      n = n+1
  print("For label {0} and chunk_time {1}".format(label_name,chunk_time))
  print("Train-accuracy Test-accuracy Train-PCC Test-PCC")
  print("{0}±{1} {2}±{3} {4}±{5} {6}±{7}".format(round(np.array(train_acc).mean(),3),round(np.array(train_acc).std(),2), round(np.array(test_acc).mean(),3),round(np.array(test_acc).std(),2),round(np.array(train_PCC).mean(),3),round(np.array(train_PCC).std(),2),round(np.array(test_PCC).mean(),3),round(np.array(test_PCC).std(),2)))

label = 'Overall'

chunk_time = 5
feature_size = 23

Data_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/Data_" + str(chunk_time)+".csv"
Label_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/" + str(label) + "_" + str(chunk_time)+".csv"

Data = pd.read_csv(Data_path, header = None)
Data = Data.fillna(method='ffill')
Data = np.asarray(Data)
Label = np.asarray(pd.read_csv(Label_path, header = None))
np.random.shuffle(Label)

scaler = preprocessing.StandardScaler().fit(Data)
X_scaled = scaler.transform(Data)
Data_final = X_scaled
model_call(Data_final,Label,chunk_time,feature_size,label)

chunk_time = 10
feature_size = 23

Data_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/Data_" + str(chunk_time)+".csv"
Label_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/" + str(label) + "_" + str(chunk_time)+".csv"

Data = pd.read_csv(Data_path, header = None)
Data = Data.fillna(method='ffill')
Data = np.asarray(Data)
Label = np.asarray(pd.read_csv(Label_path, header = None))
np.random.shuffle(Label)

scaler = preprocessing.StandardScaler().fit(Data)
X_scaled = scaler.transform(Data)
Data_final = X_scaled
model_call(Data_final,Label,chunk_time,feature_size,label)

chunk_time = 15
feature_size = 23

Data_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/Data_" + str(chunk_time)+".csv"
Label_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/" + str(label) + "_" + str(chunk_time)+".csv"

Data = pd.read_csv(Data_path, header = None)
Data = Data.fillna(method='ffill')
Data = np.asarray(Data)
Label = np.asarray(pd.read_csv(Label_path, header = None))
np.random.shuffle(Label)

scaler = preprocessing.StandardScaler().fit(Data)
X_scaled = scaler.transform(Data)
Data_final = X_scaled
model_call(Data_final,Label,chunk_time,feature_size,label)

chunk_time = 30
feature_size = 23

Data_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/Data_" + str(chunk_time)+".csv"
Label_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/" + str(label) + "_" + str(chunk_time)+".csv"

Data = pd.read_csv(Data_path, header = None)
Data = Data.fillna(method='ffill')
Data = np.asarray(Data)
Label = np.asarray(pd.read_csv(Label_path, header = None))
np.random.shuffle(Label)

scaler = preprocessing.StandardScaler().fit(Data)
X_scaled = scaler.transform(Data)
Data_final = X_scaled
model_call(Data_final,Label,chunk_time,feature_size,label)

chunk_time = 60
feature_size = 23

Data_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/Data_" + str(chunk_time)+".csv"
Label_path = "/content/drive/MyDrive/Chunk_Data_Local_Features/" + str(label) + "_" + str(chunk_time)+".csv"

Data = pd.read_csv(Data_path, header = None)
Data = Data.fillna(method='ffill')
Data = np.asarray(Data)
Label = np.asarray(pd.read_csv(Label_path, header = None))
np.random.shuffle(Label)

scaler = preprocessing.StandardScaler().fit(Data)
X_scaled = scaler.transform(Data)
Data_final = X_scaled
model_call(Data_final,Label,chunk_time,feature_size,label)