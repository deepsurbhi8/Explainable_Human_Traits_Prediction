# -*- coding: utf-8 -*-
"""MIT_Audio_Only_Video_Level.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tUWinomTkzsKE8dqH_EERzUds9JbibqV
"""

import numpy as np
import pandas as pd
import glob
import random
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from math import sqrt
import matplotlib.pyplot as plt
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import mean_absolute_error
from sklearn import metrics
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.decomposition import PCA
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
import scipy.io as sio
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import regularizers
from tensorflow.keras.utils import to_categorical
from sklearn import preprocessing

from google.colab import drive
drive.mount('/content/drive')

def model_formation(size_of_feature_set, chunk_time):
  nKineme, seqLen, nClass = size_of_feature_set, chunk_time-1, 1
  #Model Formation
  from keras.models import Sequential
  from keras.layers import Dense
  from keras.layers import LSTM
  from keras.layers import Dropout
  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
  Model = Sequential()
  Model.add(LSTM(20,activation="tanh",dropout=0.2,recurrent_dropout=0.0,input_shape=(seqLen, size_of_feature_set)))
  Model.add(Dense(units = nClass, activation='linear'))
  opt = keras.optimizers.Adam(learning_rate=0.01)
  Model.compile(optimizer = opt, loss = 'mean_absolute_error')
  Model.summary()
  return Model

def chunks_formation(file_path, labels,chunk_time, size_of_feature_set):
    all_files = file_path
    data_list = []
    final_label = []


    label_file_index = 0 
    
    for f in all_files:
        csv_file   = pd.read_csv(f, header=None)
        csv_file = csv_file.fillna(method='ffill')
        # len_file = csv_file.shape[1]
        i = 0          
        entire_file_mat = []
        
        while i < csv_file.shape[1]:
          new_frame = csv_file.loc[:, i:i+87]
          i = i + 44
          avg_value = new_frame.mean(axis=1)
          file_mat = pd.concat([avg_value], axis=1, ignore_index=True)
          # print(file_mat.shape)
          file_mat = file_mat.to_numpy().flatten()
          file_mat = pd.DataFrame(file_mat)
          entire_file_mat = np.concatenate((entire_file_mat, file_mat), axis=None)
          # print(np.shape(entire_file_mat))
          new_array = entire_file_mat
          value_to_be_minus = new_array.shape[0]-(2*size_of_feature_set) #as due to extra data
          new_array = new_array[0:value_to_be_minus] #To handle extra 2 sec data
          
        num_chunks = int(new_array.shape[0]/(size_of_feature_set*(chunk_time-1)))
        # print("Total number of chunks:")
        # print(num_chunks)
      
        for num in range(0, num_chunks):
            data_chunk = entire_file_mat[num*(chunk_time-1)*size_of_feature_set:((num+1)*(chunk_time-1)*size_of_feature_set)]
            data_list.append(data_chunk)
        
        updated_label = labels
        one_list = updated_label[label_file_index].repeat(num_chunks)
        final_label.extend(one_list)

        
        label_file_index += 1
   
    data_array = np.array(data_list) 
    my_df = pd.DataFrame(data_array)
    Data = my_df.fillna(method='ffill')
    Data = np.asarray(Data)
    return Data,final_label

def chunks_formation_test(file_path, chunk_time, size_of_feature_set,scaler):
  f = file_path
  data_list = []
  final_label = []
  label_file_index = 0 

  # for f in all_files:
  csv_file   = pd.read_csv(f, header=None)
  csv_file = csv_file.fillna(method='ffill')
  # len_file = csv_file.shape[1]
  i = 0          
  entire_file_mat = []

  while i < csv_file.shape[1]:
    new_frame = csv_file.loc[:, i:i+87]
    i = i + 44
    avg_value = new_frame.mean(axis=1)
    file_mat = pd.concat([avg_value], axis=1, ignore_index=True)
    # print(file_mat.shape)
    file_mat = file_mat.to_numpy().flatten()
    file_mat = pd.DataFrame(file_mat)
    entire_file_mat = np.concatenate((entire_file_mat, file_mat), axis=None)
    # print(np.shape(entire_file_mat))
    new_array = entire_file_mat
    value_to_be_minus = new_array.shape[0]-(2*size_of_feature_set)
    new_array = new_array[0:value_to_be_minus] #To handle extra 2 sec data
    
  num_chunks = int(new_array.shape[0]/(size_of_feature_set*(chunk_time-1)))
  # print("Total number of chunks:")
  # print(num_chunks)

  for num in range(0, num_chunks):
      data_chunk = entire_file_mat[num*(chunk_time-1)*size_of_feature_set:((num+1)*(chunk_time-1)*size_of_feature_set)]
      data_list.append(data_chunk)

  data_array = np.asarray(data_list)
  data_df= pd.DataFrame(data_array)
  Data = data_df.fillna(method='ffill')
  Data = np.asarray(Data)
  scaled_Data = scaler.transform(Data)
  Data_final = scaled_Data
  return Data_final

def model_call_video(label_name, file_list, la_path, Model,nKineme,seqLen):
  #Creation of files
  l = label_name
  f = np.array(file_list).reshape(-1,1) #All data files
  L = pd.read_csv(la_path,low_memory=False)
  label = np.around(L[l],3) #Labels for corresponding label
  label = label.to_numpy().reshape(-1,1)  
  file_to_work = np.concatenate((f, label),axis=1)   #File plus label in single array
  # file_to_work = file_to_work[0:4,:]
  #Lists to have PCC and MAE
  train_mae=[]
  test_mae =[]
  train_PCC =[]
  test_PCC = []
  train_acc =[]
  test_acc = []
 
  randnums= np.random.randint(1,101,138)   #To make train and test fold
  n=1
  # randnums= np.random.randint(1,file_to_work.shape[0])   #To make train and test fold
  random_state = 42
  rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=random_state)     #repeat kfold function
  for train_idx, test_idx in rkf.split(randnums):
      print(n)
      train_features, test_features = file_to_work[train_idx], file_to_work[test_idx]  
      file_path = train_features[:,0].tolist()  #File list for chunk preparation
      train_labels = train_features[:,1]  #labels for chunk preparation
      X_data, y_data = chunks_formation(file_path, train_labels,chunk_time, size_of_feature_set)   #Calling of data preprocessing
      scaler = preprocessing.StandardScaler().fit(X_data)
      X_scaled = scaler.transform(X_data)
      X_data = X_scaled
      train_aus = X_data.reshape((X_data.shape[0], seqLen, nKineme))
      # print("Train Data:{0}".format(train_aus))
      # test_aus = test_features.reshape((test_features.shape[0], seqLen, nKineme))
      # print("Test Data:{0}".format(test_aus))
      # print(train_aus.shape, np.asarray(y_data).shape)
      y_data = np.asarray(y_data)
      y_data = y_data.astype('float64')
      zero_bias_history = Model.fit(train_aus, y_data, epochs = 30, batch_size = 32, validation_split=0.1,callbacks=[callback])  #Fitting the model 

      #train predictions
      y_pred_train = Model.predict(train_aus)
      y_pred_train = np.around(y_pred_train,3)

      test_data = test_features[:,0].tolist()  #test data
      test_labels = test_features[:,1].tolist()  #test actual label
      test_labels = np.array(test_labels).astype(float)
      y_pred_video = []
      for i in range(0,len(test_data)):
        X = chunks_formation_test(test_data[i], chunk_time, size_of_feature_set,scaler)
        test_features = X.reshape((X.shape[0], seqLen, nKineme))
        y_pred = Model.predict(test_features)
        #print(y_pred_video)
        y1 =y_pred.mean()  #Average predicted value of all labels
        #print(y1)
        y_pred_video.append(y1)
        #print(y_pred_video)
      y_pred_video = np.around(y_pred_video,3)

      # y_pred_test = Model.predict(test_aus)
      # y_pred_test = np.around(y_pred_test,3)
      train_mae.append(mean_absolute_error(y_data, y_pred_train)) ##mean squarred train error
      test_mae.append(mean_absolute_error(test_labels, y_pred_video)) #mean squarred test error
      train_acc.append(1-mean_absolute_error(y_data, y_pred_train))
      test_acc.append(1-mean_absolute_error(test_labels, y_pred_video))

      y_train = y_data.reshape(-1,1)
      b = np.corrcoef(y_train.T,y_pred_train.T)
      train_PCC.append(b[0][1])
      y = test_labels.reshape(-1,1)
      a = np.corrcoef(y.T,y_pred_video.T)
      test_PCC.append(a[0][1])
      # print(n)
      n = n+1
  print(np.shape(train_acc),np.shape(test_acc), np.shape(train_PCC),np.shape(test_PCC))
  print("For label {0} and chunk_time {1}".format(l,chunk_time))
  print("Train-accuracy Test-accuracy Train-PCC Test-PCC")
  print("{0}±{1} {2}±{3} {4}±{5} {6}±{7}".format(round(np.array(train_acc).mean(),3),round(np.array(train_acc).std(),2), round(np.array(test_acc).mean(),3),round(np.array(test_acc).std(),2),round(np.array(train_PCC).mean(),3),round(np.array(train_PCC).std(),2),round(np.array(test_PCC).mean(),3),round(np.array(test_PCC).std(),2)))

label_name = 'Overall'

chunk_time = 60
size_of_feature_set = 23
file_list = sorted(glob.glob('/content/drive/MyDrive/MIT_Data_CSV/*.csv'))
la_path = '/content/drive/MyDrive/labels_for_MIT.csv'
#raw_files = glob.glob('/content/drive/MyDrive/Updated Kineme Sequence/Openface_modified_by_removing_error_frames/Openface_extracted_files/*.csv')
Model = model_formation(size_of_feature_set, chunk_time)
model_call_video('Overall', file_list, la_path, Model)

chunk_time = 30
size_of_feature_set = 23
nKineme, seqLen, nClass = size_of_feature_set, chunk_time-1, 1
file_list = sorted(glob.glob('/content/drive/MyDrive/MIT_Data_CSV/*.csv'))
la_path = '/content/drive/MyDrive/labels_for_MIT.csv'
#raw_files = glob.glob('/content/drive/MyDrive/Updated Kineme Sequence/Openface_modified_by_removing_error_frames/Openface_extracted_files/*.csv')
Model = model_formation(size_of_feature_set, chunk_time)
model_call_video('Overall', file_list, la_path, Model,nKineme,seqLen)

chunk_time = 15
size_of_feature_set = 23
nKineme, seqLen, nClass = size_of_feature_set, chunk_time-1, 1
file_list = sorted(glob.glob('/content/drive/MyDrive/MIT_Data_CSV/*.csv'))
la_path = '/content/drive/MyDrive/labels_for_MIT.csv'
#raw_files = glob.glob('/content/drive/MyDrive/Updated Kineme Sequence/Openface_modified_by_removing_error_frames/Openface_extracted_files/*.csv')
Model = model_formation(size_of_feature_set, chunk_time)
model_call_video('Overall', file_list, la_path, Model,nKineme,seqLen)

chunk_time = 10
size_of_feature_set = 23
nKineme, seqLen, nClass = size_of_feature_set, chunk_time-1, 1
file_list = sorted(glob.glob('/content/drive/MyDrive/MIT_Data_CSV/*.csv'))
la_path = '/content/drive/MyDrive/labels_for_MIT.csv'
#raw_files = glob.glob('/content/drive/MyDrive/Updated Kineme Sequence/Openface_modified_by_removing_error_frames/Openface_extracted_files/*.csv')
Model = model_formation(size_of_feature_set, chunk_time)
model_call_video('Overall', file_list, la_path, Model,nKineme,seqLen)

chunk_time = 5
size_of_feature_set = 23
nKineme, seqLen, nClass = size_of_feature_set, chunk_time-1, 1
file_list = sorted(glob.glob('/content/drive/MyDrive/MIT_Data_CSV/*.csv'))
la_path = '/content/drive/MyDrive/labels_for_MIT.csv'
#raw_files = glob.glob('/content/drive/MyDrive/Updated Kineme Sequence/Openface_modified_by_removing_error_frames/Openface_extracted_files/*.csv')
Model = model_formation(size_of_feature_set, chunk_time)
model_call_video('Overall', file_list, la_path, Model,nKineme,seqLen)